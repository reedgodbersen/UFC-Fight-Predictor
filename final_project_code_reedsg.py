# -*- coding: utf-8 -*-
"""Final Project Code_reedsg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WOSw__aQ4NOp3UNMBtVzHBeCLE-got6m

# First Neural Network Model (ANN V1)
"""

import torch
import matplotlib.pyplot as plt
from torch import nn
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
from sklearn.preprocessing import StandardScaler

#for logistic model
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from google.colab import files
uploaded_files = files.upload()
#training_dataset_file = open(uploaded_files['ufc_combined.csv'], 'r')
ufc_df = pd.read_csv("ufc_combined.csv", index_col=0)
#ufc_df.head()

#categorical features

ufc_df_cont = ufc_df.drop(["winner","fighter1", "fighter2", "weight_class", "title_fight", "method", "end_round", "fight_year",
        "height_fighter1", "reach_fighter1", "stance_fighter1", "born_year_fighter1",
        "height_fighter2", "reach_fighter2", "stance_fighter2", "born_year_fighter2"], axis=1)

ufc_df_y = ufc_df["winner"]

#creating the test and train splits using sklearn
X_train, X_test, y_train, y_test = train_test_split(ufc_df_cont, ufc_df_y, test_size=0.33)

#creating scaler object
scaler = StandardScaler()
scaler.fit(X_train)
scaler.fit(X_test)

X_train = X_train.to_numpy()
X_test = X_test.to_numpy()

#standardizing the datset
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

#creating the class for the data.
class Data(nn.Module):
  def __init__(self, X_train, y_train):
    self.X = torch.from_numpy(X_train.astype(np.float32))
    self.y = torch.from_numpy(y_train).type(torch.LongTensor)
    self.len = self.X.shape[0]

  def __getitem__(self, index):
    return self.X[index], self.y[index]

  def __len__(self):
    return self.len

#creating the train and test sets to go in the dataloader.
train_data = Data(X_train, y_train)
test_data = Data(X_test, y_test)

#initializng the batch size here
batch_size = 32
#initializing the training and test dataloaders. the data loader is a class that helps store and iterate over the elements in the data sets.
trainLoader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)

#input dimension is equal to the number of features being inputted
#hidden layers is the number of neurons between the the input and output layer.
#output dimension is 1 as we are calculating the probabilities of a binary classification.

input_dim = 26
hidden_layers = 26
output_dim = 1

class Network(nn.Module):
  def __init__(self):
    super(Network, self).__init__()
    self.linear1 = nn.Linear(input_dim, hidden_layers)
    self.linear2 = nn.Linear(hidden_layers, output_dim)

  def forward(self, x):
    x = self.linear1(x)
    x = torch.sigmoid(x)
    x = self.linear2(x)
    x = torch.sigmoid(x)

    return x

#creating an instance of the network object
clf = Network()

#i have found printing the parameters useful as a sanity check after making changes to the network class.
print(clf.parameters)

#criterion is the loss functon, which in this case is binary cross entropy loss.
#the lr argument in the optimizer is the learning rate
criterion = nn.BCELoss()
optimizer = torch.optim.SGD(clf.parameters(), lr=0.01)

#neural network gets trained here. running loss is kept track of and then plotted.
epochs = 16
running_loss_list = []
for epoch in range(epochs):
  running_loss = 0.0

  for i, data in enumerate(trainLoader, 0):
    inputs, labels = data
    labels = labels.to(torch.float32)
    optimizer.zero_grad()
    outputs = clf(inputs)
    outputs = outputs.squeeze()
    #print(outputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    #print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')

    running_loss_list.append(loss.item())
    #epoch_list.append(epoch + 1)


#plotting loss
x = np.arange(1, len(running_loss_list) + 1)
plt.plot(x, running_loss_list)
print(running_loss_list)
plt.xlim(0,1750)
plt.ylim(0.4,1)
plt.xlabel('Number of Loss Calculations in Training')
plt.ylabel('Training Loss')
plt.show()

#prepares test data
testLoader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)

#iterates through the testloader
dataiter = iter(testLoader)
inputs, labels = next(dataiter)
print(inputs.shape)
print(labels.shape)

#setting outputs equal to the trained model
outputs = clf(inputs)
print(outputs.shape)

#caluclating accuracy on the test set
correct, total = 0, 0
with torch.no_grad():
    for inputs, labels in dataiter:
      outputs = clf(inputs)
      predicted = outputs
      predicted[predicted > 0.5] = 1
      predicted[predicted <=0.5] = 0
      total += labels.size(0)

      correct += (predicted == labels.unsqueeze(1)).sum().sum().item()
accuracy = correct / total
#print(f'Accuracy of ann on {len(test_data)} test data: {100 * correct // total} %')
print("Accuracy: {:.2f}%".format(100*correct/total))

"""# Logistic Regression Model"""

model = LogisticRegression()

#creating the test and train splits using sklearn
X_train, X_test, y_train, y_test = train_test_split(ufc_df_cont, ufc_df_y, test_size=0.33)
#trying without random state in split to see any changes

#creating scaler object
scaler = StandardScaler()
scaler.fit(X_train)
scaler.fit(X_test)

X_train = X_train.to_numpy()
X_test = X_test.to_numpy()
#standardize these guys
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
y_train = y_train.to_numpy()
y_test = y_test.to_numpy()

model.fit(X_train, y_train)

logistic_prediction = model.predict(X_test)

#calculating accuracy
accuracy = accuracy_score(y_test, logistic_prediction)
print("Accuracy:", accuracy)

#calculating precision
precision = precision_score(y_test, logistic_prediction)
print(precision)

#creating confusion matrix
c_matrix = confusion_matrix(y_test, logistic_prediction)

#creating the sns heatmap for the confusion matrix.
sns.heatmap(c_matrix/np.sum(c_matrix), annot=True, cmap="Blues", fmt='.2%')
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title(f"Confusion Matrix\nPrecision: {precision:.2f}")
plt.show()

#roc curve

fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])

plt.plot(fpr, tpr)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')

"""# Second Neural Network Model

"""

#creating the class for the data, as done in the first ann
class Data2(nn.Module):
  def __init__(self, X_train, y_train):
    self.X = torch.from_numpy(X_train.astype(np.float32))
    self.y = torch.from_numpy(y_train).type(torch.LongTensor)
    self.len = self.X.shape[0]

  def __getitem__(self, index):
    return self.X[index], self.y[index]

  def __len__(self):
    return self.len

#initializng the batch size here
batch_size = 32
#initializing the training and test dataloaders. the data loader is a class that helps store and iterate over the elements in the data sets.
trainLoader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)

train_data = Data(X_train, y_train)
test_data = Data(X_test, y_test)

#creating Network class, this time with a relu function to introduce nonlinearity.

input_dim = 26
hidden_layers = 26
output_dim = 1

class Network(nn.Module):
  def __init__(self):
    super(Network, self).__init__()
    self.linear1 = nn.Linear(input_dim, hidden_layers)
    self.linear2 = nn.Linear(hidden_layers, output_dim)

  def forward(self, x):
    x = self.linear1(x)
    x = torch.relu(x)
    x = self.linear2(x)
    x = torch.relu(x)

    return x

#creating an instance of the network class
clf_relu = Network()

criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(clf.parameters(), lr=0.0001)

#add term to loss: the closer the pred is to .5, the more u add to the loss
#make logistic regression to compare to neural netwwork
epochs = 12
running_loss_list = []
for epoch in range(epochs):
  running_loss = 0.0

  for i, data in enumerate(trainLoader, 0):
    inputs, labels = data
    labels = labels.to(torch.float32)
    optimizer.zero_grad()
    outputs = clf_relu(inputs)
    outputs = outputs.squeeze()
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    #print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')

    running_loss_list.append(loss.item())

#plotting the running loss
x = np.arange(1, len(running_loss_list) + 1)
plt.plot(x, running_loss_list)
print(running_loss_list)
plt.xlim(0,1400)
plt.ylim(0,1)
plt.show()

#initializing test dataloaders. the data loader is a class that helps store and iterate over the elements in the data sets.
testLoader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)
dataiter = iter(testLoader)
inputs, labels = next(dataiter)

#caluclating accuracy on the test set
correct, total = 0, 0
with torch.no_grad():
    for inputs, labels in dataiter:
      outputs = clf_relu(inputs)
      predicted = outputs
      predicted[predicted > 0.5] = 1
      predicted[predicted <=0.5] = 0
      total += labels.size(0)

      correct += (predicted == labels.unsqueeze(1)).sum().sum().item()
accuracy = correct / total
#print(f'Accuracy of ann on {len(test_data)} test data: {100 * correct // total} %')
print("Accuracy: {:.2f}%".format(100*correct/total))

"""# ANN V1 Accuracy: 66.61%
# ANN V2 Accuracy: 51.24%
# Logistic Regression Accuracy:70%

# Stacking more hidden layers +  ReLu activation. This idea was abanded due to it being out of my skill level and time, but I reference in my report conclusion that this would be a worthwhile endeavor to increasing the learning of the neural network.
"""

class Data3(nn.Module):
  def __init__(self, X_train, y_train):
    self.X = torch.from_numpy(X_train.astype(np.float32))
    self.y = torch.from_numpy(y_train).type(torch.LongTensor)
    self.len = self.X.shape[0]

  def __getitem__(self, index):
    return self.X[index], self.y[index]

  def __len__(self):
    return self.len

input_dim = 26
hidden_layers = 26
output_dim = 1
hidden_dims = [2, 2]

class HiddenLayers(nn.Module):
  def __init__(self, input_dim, hidden_dims, output_dim):


    return x

input_dim = 26
hidden_dim = 26
output_dim = 1
class HiddenLayers(nn.Module):
  def __init__(self, input_dim, hidden_dim, output_dim):
    super(HiddenLayers, self).__init__()
    self.fc1 = nn.Linear(input_dim, hidden_dim)
    self.fc2 = nn.Linear(hidden_dim, hidden_dim)
    self.fc3 = nn.Linear(hidden_dim, hidden_dim)
    self.fc4 = nn.Linear(hidden_dim, output_dim)
    self.activation = nn.ReLU()

  def forward(self, x):
    x = self.activation(self.fc1(x))
    x = self.activation(self.fc2(x))
    x = self.activation(self.fc3(x))
    x = self.fc4(x)
    return x

hl = HiddenLayers(input_dim, hidden_dims, output_dim)

criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(clf.parameters(), lr=0.01)

#add term to loss: the closer the pred is to .5, the more u add to the loss
#make logistic regression to compare to neural netwwork
epochs = 4
running_loss_list = []
#epoch_list = []
for epoch in range(epochs):
  running_loss = 0.0

  for i, data in enumerate(trainLoader, 0):
    inputs, labels = data
    labels = labels.to(torch.float32)
    optimizer.zero_grad()
    outputs = hl(inputs)
    outputs = outputs.squeeze()
    #print(outputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    #print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')

    running_loss_list.append(loss.item())
    #epoch_list.append(epoch + 1)


x = np.arange(1, len(running_loss_list) + 1)
plt.plot(x, running_loss_list)
print(running_loss_list)
plt.xlim(0,500)
plt.ylim(0,1)
plt.show()